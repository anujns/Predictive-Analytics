# -*- coding: utf-8 -*-
"""DIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pwhfo9m2RBVc3I2g-tsX9CRyexWJTbVn
"""

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
"""
Predicitve_Analytics.py
"""
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 


from sklearn.model_selection import train_test_split
from scipy import stats

import time


from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import GridSearchCV

def zscorenormalisation(X_train):
    sta_vec = np.std(X_train ,axis = 0)
    mean_vec = np.mean(X_train, axis = 0)
    
    for row in range(X_train.shape[0]):
        for col in range(X_train.shape[1]):
            X_train[row][col] = (X_train[row][col] - mean_vec[col])/sta_vec[col]
    
    return X_train

def import_data(filename):
    df = pd.read_csv(filename)
    return df

df = import_data('data.csv')

input_data = df.values

X = input_data[:, :input_data.shape[1]-1]
y = input_data[:, input_data.shape[1]-1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train = zscorenormalisation(X_train)
X_test = zscorenormalisation(X_test)

k = 10

def find_center(cluster_centers, row, k):
    
    rows = np.tile(row, (k,1))
    
#     diff_vector = np.sum(np.absolute(cluster_centers - rows), axis = 1)
    diff_vector = np.sqrt(np.sum(np.square(cluster_centers - rows), axis = 1))
    
    return np.argmin(diff_vector)

def Accuracy(y_true,y_pred):
    """
    :type y_true: numpy.ndarray
    :type y_pred: numpy.ndarray
    :rtype: float
    
    """
    diff = y_true - y_pred
    t = np.where(diff == 0)
    return t[0].shape[0]/y_true.shape[0]

def Recall(y_true,y_pred):
    """
    :type y_true: numpy.ndarray
    :type y_pred: numpy.ndarray
    :rtype: float
    """
    labels = [1,2,3,4,5,6,7,8,9,10,11]
    macro = []
    tp = [0 for x in labels]
    fn = [0 for x in labels]
    for i in range(len(labels)):
        for x in range(y_true.shape[0]):
            if y_true[x] == labels[i]:
                if y_pred[x] == y_true[x]:
                    tp[i] = tp[i] + 1
                else:
                    fn[i] = fn[i] + 1
        

    
    return np.sum(tp)/(np.sum(tp)+np.sum(fn))

def Precision(y_true,y_pred):
    """
    :type y_true: numpy.ndarray
    :type y_pred: numpy.ndarray
    :rtype: float
    """
    labels = [1,2,3,4,5,6,7,8,9,10,11]
    macro = []
    tp = [0 for x in labels]
    fp = [0 for x in labels]
    for i in range(len(labels)):
        actual = y_true
        pred = y_pred
        
        act_diff = actual - labels[i]
        pred_diff = pred - labels[i]
        
        for x in range(y_true.shape[0]):
            if y_pred[x] == labels[i]:
                if y_pred[x] == y_true[x]:
                    tp[i] = tp[i] + 1
                else:
                    fp[i] = fp[i] + 1
    
    return np.sum(tp)/(np.sum(tp)+np.sum(fp))

def WCSS(Clusters):
    """
    :Clusters List[numpy.ndarray]
    """
    wcss = 0
    # Get a cluster
    for cluster in Clusters:
        
        cluster_center = np.mean(cluster, axis = 0)
        
        # Compute all distances from cluster center.
        center_vector = np.tile(cluster_center, (cluster.shape[0], 1))
        
        
        wcss = wcss + np.sum(np.sqrt(np.sum(np.square(center_vector - cluster), axis = 1)))
#         wcss = wcss + np.sum(np.absolute(center_vector - cluster))
    
    return wcss

def ConfusionMatrix(y_true,y_pred):
    """
    :type y_true: numpy.ndarray
    :type y_pred: numpy.ndarray
    :rtype: float
    """  
    cf_matrix = np.zeros((11,11))
    y_true = y_true.astype(int)
    y_pred = y_pred.astype(int)
    
    for x in range(y_true.shape[0]):
        cf_matrix[y_true[x]-1][y_pred[x]-1] = cf_matrix[y_true[x]-1][y_pred[x]-1] + 1
    
    return cf_matrix

def KNN(X_train,X_test,Y_train):
    """
    :type X_train: numpy.ndarray
    :type X_test: numpy.ndarray
    :type Y_train: numpy.ndarray
    
    :rtype: numpy.ndarray
    """
    k=10
    labels = np.zeros(X_test.shape[0])
    for x in range(X_test.shape[0]):
        row = np.tile(X_test[x], (X_train.shape[0],1))
        diff_matrix = np.sum(np.absolute(X_train - row), axis=1)
        labelled_matrix = np.vstack((diff_matrix, Y_train))
        labelled_matrix = labelled_matrix.T
        sorted_matrix = labelled_matrix[labelled_matrix[:,0].argsort()]
        top_k = sorted_matrix[:k]
        labels[x] = stats.mode(top_k[:,1])[0]
    return labels

def RandomForest(X_train,Y_train,X_test):
    """
    :type X_train: numpy.ndarray
    :type X_test: numpy.ndarray
    :type Y_train: numpy.ndarray
    
    :rtype: numpy.ndarray
    """
    clf = RandomForest(n_trees=3, max_depth=10)
    clf.fit(X_train, Y_train)
    y_pred = clf.predict(X_test)
    return y_pred

def PCA(X_train,N):
    """
    :type X_train: numpy.ndarray
    :type N: int
    :rtype: numpy.ndarray
    """
    Covariance = np.dot(X_train.T, X_train) / (N-1)
    __ , vector = np.linalg.eig(Covariance)
    return np.dot(X_train, vector)

def Kmeans(X_train,N):
    """
    :type X_train: numpy.ndarray
    :type N: int
    :rtype: List[numpy.ndarray]
    """
    
    cluster_centers = X_train[np.random.choice(X_train.shape[0], size = N), ]
    clusters = [np.zeros((1,X_train.shape[1])) for x in range(N)]
    
    epochs = 10
    
    for epoch in range(epochs):
        print(epoch)
        clusters = [np.zeros((1,X_train.shape[1])) for x in range(N)]
        
        print('starting main loop')
        for x in range(X_train.shape[0]):
            center_index = find_center(cluster_centers, X_train[x], N)

            # Adding the new point to the cluster.        
            clusters[center_index] = np.vstack((clusters[center_index], X_train[x]))
        
        print('recomputing center.')
        
        # Recomputing the center.
        for x in range(cluster_centers.shape[0]): 
            cluster_centers[x] = np.mean(clusters[x], axis = 0)
        
        print('done computing centers.')

    
    return clusters

def SklearnSupervisedLearning(X_train,Y_train,X_test, Y_test):
    """
    :type X_train: numpy.ndarray
    :type X_test: numpy.ndarray
    :type Y_train: numpy.ndarray
    
    :rtype: List[numpy.ndarray] 
    """
    results = []
    
    # Scaling
    sc = MinMaxScaler()
    x_train = sc.fit_transform(X_train)
    x_test = sc.transform(X_test)
    
    # Training SVM
    
    svc_clf=SVC(kernel ='linear', C=1, gamma=1)
    svc_clf.fit(X_train, Y_train)
    y_pred_svc= svc_clf.predict(X_test)
    print("SVM Accuracy: " + str(accuracy_score(Y_test, y_pred_svc) * 100))
    
        
    # confusion matrix - SVM
    cm_svc = confusion_matrix(Y_test, y_pred_svc)
    print('Confusion matrix for SVM.')
    print(cm_svc)
    plt.matshow(cm_svc)
    plt.title('Confusion matrix of the classifier')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    
    
    # Training KNN
    knn_model = KNeighborsClassifier(n_neighbors=5)
    knn_model.fit(X_train, Y_train)
    y_pred_knn = knn_model.predict(X_test)
    print("KNN Accuracy: " + str(accuracy_score(Y_test, y_pred_knn) * 100))
    
    
    # Confusion Matrix - KNN
    cm_knn = confusion_matrix(y_test, y_pred_knn)
    print('Confusion matrix for KNN.')
    print(cm_knn)
    plt.matshow(cm_knn)
    plt.title('Confusion matrix of the classifier')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    
    
    # Decision Tree
    dt = DecisionTreeClassifier(max_leaf_nodes=50, random_state=0)
    dt.fit(X_train, Y_train)
    y_pred_tree = dt.predict(X_test)
    print("Decision Tree Accuracy: " + str(accuracy_score(Y_test, y_pred_tree) * 100))
    
    # Confusion Matrix - DTree
    cm_tree = confusion_matrix(y_test, y_pred_tree)
    print('Confusion matrix for Decision Tree.')
    print(cm_tree)

    plt.matshow(cm_tree)
    plt.title('Confusion matrix of the classifier')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    
    # Logistic Regression Model
    logclf = LogisticRegression(random_state = 0, penalty = 'l1', solver='saga', class_weight='balanced', multi_class='multinomial')
    logclf.fit(X_train, Y_train)
    y_pred_log = logclf.predict(X_test)
    print("Logistic Regression Accuracy: " + str(accuracy_score(Y_test, y_pred_log) * 100))
    
    cm_lr = confusion_matrix(Y_test, y_pred_log)
    print('Confusion matrix for Logistic Regression.')
    print(cm_lr)
    plt.matshow(cm_lr)
    plt.title('Confusion matrix of the classifier')
    plt.xlabel('Predicted')
    plt.ylabel('True')


    results.append(y_pred_svc)
    results.append(y_pred_knn)
    results.append(y_pred_tree)
    results.append(y_pred_log)
    
    return results

def SklearnVotingClassifier(X_train,Y_train,X_test, Y_test):
    
    """
    :type X_train: numpy.ndarray
    :type X_test: numpy.ndarray
    :type Y_train: numpy.ndarray
    
    :rtype: List[numpy.ndarray] 
    """
    logclf = LogisticRegression(random_state = 0, penalty = 'l1', solver='saga', class_weight='balanced', multi_class='multinomial')
    logclf.fit(X_train, Y_train)
    dt = DecisionTreeClassifier(max_leaf_nodes=50, random_state=0)
    svc_clf=SVC(kernel ='linear', C=1, gamma=1)
    knn_model = KNeighborsClassifier(n_neighbors=5)
    
    eclf1 = VotingClassifier(estimators=[('lr', logclf), ('dt', dt), ('knn', knn_model), ('svc', svc_clf)], voting='hard')
    eclf1 = eclf1.fit(X_train, Y_train)
    y_ens = logclf.predict(X_test)
    print("Ensemble Model Accuracy: " + str(accuracy_score(Y_test, y_ens) * 100))
    return y_ens

def bootstrap_sample(X, y):
    idxs = np.random.choice(X.shape[0], X.shape[0], replace=True)
    return X[idxs], y[idxs]

def most_common_label(y):  
    return stats.mode(y)[0]


class RandomForest:
    
    def __init__(self, n_trees=10, min_samples_split=2,
                 max_depth=100, n_feats=None):
        self.n_trees = n_trees
        self.minimum_samples = min_samples_split
        self.max_depth = max_depth
        self.feature_count = n_feats
        self.trees = []

    def predict(self, X_train):
        pred = []
  
        for t in self.trees:
            pred.append(t.predict(X_train))


        pred = np.array(pred)
        pred = np.swapaxes(pred, 0, 1)


        output = []
        for p in pred:
            output.append(most_common_label(p))     
        return np.array(output)

    def fit(self, X_train, y_train):
        self.trees = []
        for _ in range(self.n_trees):
            dt = DecisionTree(min_samples_split=self.minimum_samples,
                max_depth=self.max_depth, n_feats=self.feature_count)
            X_samp, y_samp = bootstrap_sample(X_train, y_train)
            dt.fit(X_samp, y_samp)
            self.trees.append(dt)

def entropy(y):
    y = y.astype(int)
    hist = np.bincount(y)
    ps = hist / y.shape[0]
    return -np.sum([p * np.log2(p) for p in ps if p > 0])


class Node:

    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

    def is_leaf_node(self):
        return self.value is not None


class DecisionTree:

    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.n_feats = n_feats
        self.root = None

    def fit(self, X, y):
        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])
        self.root = self.construct_tree(X, y)


    def construct_tree(self, X_train, y_train, depth=0):
        print('Grow tree -- start')
        start_time = time.time() 

        if (depth >= self.max_depth
                or np.unique(y_train).shape[0] == 1
                or X.shape[0] < self.min_samples_split):
            return Node(value=stats.mode(y_train)[0])

        optimal_feature, optimal_threshold = self.optimal_features(X_train, y_train, np.random.choice(X.shape[1], self.n_feats, replace=False))

        lindex, rindex = self._split(X_train[:, optimal_feature], optimal_threshold)

        left_tree = self.construct_tree(X_train[lindex, :], y_train[lindex], depth+1)
        right_tree = self.construct_tree(X_train[rindex, :], y_train[rindex], depth+1)
        
        print("--- %s seconds ---" % (time.time() - start_time))

        return Node(optimal_feature, optimal_threshold, left_tree, right_tree)

    def optimal_features(self, X_train, y_train, f_index):
        max_gain = -1
        s_id, s_t = None, None
        for fid in f_index:
            col = X_train[:, fid] 
            for t in np.unique(col):
                g = self.ig(y_train, col, t)
                if g > max_gain:
                    max_gain = g
                    s_id = fid
                    s_t = t

        return s_id, s_t


    def predict(self, X):
        return np.array([self.traverse(x, self.root) for x in X])

    def ig(self, y_train, X, thres):
        l, r = self._split(X, thres)
        if len(l) == 0:
          return 0
        if len(r) == 0:
          return 0
        return entropy(y) - ((len(l) / len(y_train)) * entropy(y[l]) + (len(r) / len(y_train)) * entropy(y[r]))

    def _split(self, X_tree, thres):
        return np.argwhere(X_tree <= thres).flatten(), np.argwhere(X_tree > thres).flatten()

    def traverse(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature] <= node.threshold:
            return self.traverse(x, node.left)
        return self.traverse(x, node.right)

# Commented out IPython magic to ensure Python compatibility.
"""
Create your own custom functions for Matplotlib visualization of hyperparameter search. 
Make sure that plots are labeled and proper legends are used
""" 
# %matplotlib inline

def GridSearchSVM(X_train, Y_train):
    
    # Grid Search - SVM
    parameters = {'kernel':['linear'], 'C':[0.00001, 0.0001,0.001,0.01,0.1, 0.5, 0.8]}
    # parameters = {'kernel':['linear'], 'C':[0.1,10,20,40,70,100,200,260,512]}
    svc_clf=SVC(kernel ='linear', C=1, gamma=1)
    svc_clf.fit(X_train, Y_train)
    
    clfGridSV = GridSearchCV(svc_clf,parameters,cv=3)
    clfGridSV.fit(X_train,Y_train)
    accuracy_SVM=clfGridSV.cv_results_['mean_test_score']
    fig = plt.figure()
    plt.ylabel('Accuracy Of Linear Kernel SVM')
    plt.xlabel('Regularization Parameter (C)')
    plt.plot([0.00001, 0.0001,0.001,0.01,0.1, 0.5, 0.8],accuracy_SVM)
   
    
def GridSearchdDTree(X_train, Y_train):
    
    # Grid Search - DTree
    parameters = {'max_depth': [3,6,9,12]}
    dt = DecisionTreeClassifier(max_leaf_nodes=50, random_state=0)
    dt.fit(X_train, Y_train)
    gs_tree = GridSearchCV(dt,parameters,cv=3)
    gs_tree.fit(X_train,Y_train)
    accuracy_gs_tree=gs_tree.cv_results_['mean_test_score']
    fig = plt.figure()
    
    plt.ylabel('Accuracy Of Decision Tree')
    plt.xlabel('Regularization Parameter (max_depth)')
    plt.plot([3,6,9,12],accuracy_gs_tree)

def GridSearchKNN(X_train, Y_train):
    
    # Grid Search - KNN
    parameters = {'n_neighbors': [1,3,5,10,15,30]}
    knn_model = KNeighborsClassifier(n_neighbors=5)
    knn_model.fit(X_train, Y_train)
    gs_knn = GridSearchCV(knn_model,parameters,cv=5)
    gs_knn.fit(X_train,Y_train)
    accuracy_gs_knn=gs_knn.cv_results_['mean_test_score']
    fig = plt.figure()
    plt.ylabel('Accuracy Of KNN')
    plt.xlabel('Regularization Parameter (n_neighbor)')
    plt.plot([1,3,5,10,15,30],accuracy_gs_knn)
